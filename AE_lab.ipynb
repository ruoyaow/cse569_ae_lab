{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smTGXbCzsLhL"
   },
   "source": [
    "Adversarial Examples Lab\n",
    "==============================\n",
    "\n",
    "This lab focus on showing you an example how to create Adversarial Example towards image classifier. To be specific, we will use Fast Gradient Sign Attack to exploit the vulnerability of an MNIST classifier.\n",
    "\n",
    "Through this lab, one can find that adding imperceptible perturbations to an image can cause drastically different model performance.\n",
    "\n",
    "Credit: [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqDkIr93sLhN"
   },
   "source": [
    "### Threat Model ###\n",
    "\n",
    "A white-box attack assumes the attacker has full knowledge and access to the model, including architecture, inputs, outputs, and weights. A black-box attack assumes the attacker only has access to the inputs and outputs of the model, and knows nothing about the underlying architecture or weights. \n",
    "\n",
    "In this Lab, we are focusing on white-box attack with the goal of misclassification.\n",
    "\n",
    "### Attack Method ###\n",
    "\n",
    "We're using Fast Gradient Sign Attack ([Paper](https://arxiv.org/abs/1412.6572), [Blog](https://jaketae.github.io/study/fgsm/)) as our attack method.(The famous panda example is describing this methods too!) For the detail of this method, please look into the paper or the blog linked above.\n",
    "\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/fgsm_panda_image.png)\n",
    "\n",
    "Let's start!\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkADGaLSsLhO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elioe4EHsLhO"
   },
   "source": [
    "## Define inputs\n",
    "\n",
    "-   `epsilons` - List of epsilon values to use for the run. `epsilons` are the $\\epsilon$ described in the FGSM attack method. Note that $\\epsilon=0$ case represents the original test accuracy, with no attack.\n",
    "-   `pretrained_model` - path to the pretrained MNIST model from PyTorch, which will be downloaded under the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKjNugR3sLhP",
    "outputId": "fe6e3124-0ade-4335-c663-e81a22f711b5"
   },
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"./pretrained_model/lenet_mnist_model.pth\"\n",
    "\n",
    "os.makedirs('./pretrained_model', exist_ok=True)\n",
    "url = 'https://drive.google.com/uc?id=1HJV2nUHJqclXQ8flKvcWmjZ-OU5DGatl'\n",
    "gdown.download(url, pretrained_model, quiet=False)\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoJUvvHksLhP"
   },
   "source": [
    "## Model Under Attack\n",
    "\n",
    "The model under attack is the same MNIST model from [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist).\n",
    "\n",
    "In this section, we define the model and dataloader, then initialize the model and load the pretrained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Ryg3fX1CsLhQ",
    "outputId": "2b09f17c-6bb0-4f9c-fd86-0ec3b3f566c9"
   },
   "outputs": [],
   "source": [
    "# LeNet Model definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# MNIST Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ])),\n",
    "        batch_size=1, shuffle=True)\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Initialize the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=device, weights_only=True))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuAqSmUssLhQ"
   },
   "source": [
    "## FGSM Attack\n",
    "\n",
    "Now, we can define the function that creates the adversarial examples by\n",
    "perturbing the original inputs. The `fgsm_attack` function takes three\n",
    "inputs, *image* is the original clean image ($x$), *epsilon* is the\n",
    "pixel-wise perturbation amount ($\\epsilon$), and *data\\_grad* is\n",
    "gradient of the loss w.r.t the input image\n",
    "($\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)$). The function then\n",
    "creates perturbed image as\n",
    "\n",
    "$$perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))$$\n",
    "\n",
    "Finally, in order to maintain the original range of the data, the\n",
    "perturbed image is clipped to range $[0,1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFfXb-R6sLhR"
   },
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "# restores the tensors to their original scale\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFimIhUsLhR"
   },
   "source": [
    "We define a wrapper function to launch FGSM attack on a test dataset against the model we passed. This function returns the attacked accuracy and some successful adversarial examples to be visualized later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKAM3ocbsLhR"
   },
   "outputs": [],
   "source": [
    "def fgsm_attack_launcher(model, device, test_loader, epsilon):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in tqdm(test_loader, desc=f\"Launching attack with epsilon={epsilon}\"):\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect ``datagrad``\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Restore the data to its original scale\n",
    "        data_denorm = denorm(data)\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "        # Reapply normalization\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data_normalized)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if epsilon == 0 and len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9QHy22FsLhS"
   },
   "source": [
    "## Launch Attack\n",
    "\n",
    "In this section, we run a full test step for each epsilon value in the *epsilons* input. For each epsilon we also save the final accuracy and some successful adversarial examples to be plotted in the coming sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNROdCB7sLhS"
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Launch attack for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = fgsm_attack_launcher(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgDD4ykjsLhS"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "- Accuracy vs Epsilon\n",
    "\n",
    "First, let's look into the accuracy versus epsilon plot. As epsilon increases, we expect the test accuracy to decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssokLSnSsLhT"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AQhGsDMsLhT"
   },
   "source": [
    "- Sample Adversarial Examples\n",
    "\n",
    "As epsilon increases, the test accuracy decreases but the perturbations become more easily perceptible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4Pl1cdZsLhT"
   },
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(f\"{orig} -> {adv}\")\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
